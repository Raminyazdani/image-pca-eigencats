{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image PCA & Eigencats Analysis\n",
        "\n",
        "Principal Component Analysis applied to cat face images, computing eigencats that represent the most significant variations in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n",
        "\n",
        "Helper functions for visualization and data processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_cats(cats, layout=(2, 2), title=None):\n",
        "    \"\"\"Plot multiple cat images in a grid layout.\n",
        "    \n",
        "    Args:\n",
        "        cats: Array of shape (n_images, height, width)\n",
        "        layout: Tuple of (rows, cols) for subplot grid\n",
        "        title: Optional title for the figure\n",
        "    \"\"\"\n",
        "    rows, cols = layout\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
        "    axes = axes.flatten() if rows * cols > 1 else [axes]\n",
        "    \n",
        "    for idx, ax in enumerate(axes):\n",
        "        if idx < len(cats):\n",
        "            ax.imshow(cats[idx], cmap='gray')\n",
        "            ax.axis('off')\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "    \n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sort_eigvectors(eigenvalues, eigenvectors):\n",
        "    \"\"\"Sort eigenvectors by eigenvalues in descending order.\n",
        "    \n",
        "    Args:\n",
        "        eigenvalues: Array of eigenvalues\n",
        "        eigenvectors: Matrix of eigenvectors\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (sorted_eigenvalues, sorted_eigenvectors)\n",
        "    \"\"\"\n",
        "    idx = np.argsort(eigenvalues)[::-1]\n",
        "    return eigenvalues[idx], eigenvectors[:, idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "\n",
        "Calculate the average of all cat images to find the mean cat, then mean-center the dataset. Finally, flatten each image into a vector, resulting in 80 4096-dimensional vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(cats):\n",
        "    \"\"\"Preprocess cat images for PCA.\n",
        "    \n",
        "    Args:\n",
        "        cats: Array of shape (n_images, height, width)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (mean_cat, centered_flat) where:\n",
        "        - mean_cat: Mean image (height, width)\n",
        "        - centered_flat: Mean-centered flattened images (n_images, height*width)\n",
        "    \"\"\"\n",
        "    mean_cat = cats.mean(axis=0)\n",
        "    centered = cats - mean_cat\n",
        "    centered_flat = centered.reshape(len(cats), -1)\n",
        "    return mean_cat, centered_flat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigenvalue Decomposition (EVD)\n",
        "\n",
        "This section computes the covariance matrix and performs eigendecomposition to find the principal components. Eigenvectors are sorted in descending order by eigenvalues, and the first four eigencats are visualized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eigencats_evd(centered_flat):\n",
        "    \"\"\"Compute eigencats using eigenvalue decomposition.\n",
        "    \n",
        "    Args:\n",
        "        centered_flat: Mean-centered flattened images (n_images, n_pixels)\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (eigenvalues, eigenvectors)\n",
        "    \"\"\"\n",
        "    # Compute covariance matrix\n",
        "    cov = np.cov(centered_flat.T)\n",
        "    \n",
        "    # Eigenvalue decomposition\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
        "    \n",
        "    # Sort by eigenvalues\n",
        "    eigenvalues, eigenvectors = sort_eigvectors(eigenvalues, eigenvectors)\n",
        "    \n",
        "    return eigenvalues.real, eigenvectors.real"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Usage (with placeholder data)\n",
        "\n",
        "This demonstrates how the functions would be used once we have the actual dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder for future data loading\n",
        "# cats = np.load('cats.npy')\n",
        "# mean_cat, centered_flat = preprocess(cats)\n",
        "# eigenvalues, eigenvectors = eigencats_evd(centered_flat)\n",
        "# \n",
        "# # Visualize first 4 eigencats\n",
        "# eigencats = eigenvectors[:, :4].T.reshape(4, 64, 64)\n",
        "# plot_cats(eigencats, layout=(2, 2), title='First 4 Eigencats (EVD)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Singular Value Decomposition (SVD)\n",
        "\n",
        "This section computes eigencats using singular value decomposition, an alternative approach that is often more numerically stable than eigendecomposition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eigencats_svd(cats):    \"\"\"Compute eigencats using singular value decomposition.        Args:        cats: Array of shape (n_images, height, width)        Returns:        Tuple of (singular_values, eigenvectors)    \"\"\"    # Mean-center the data    mean_cat = cats.mean(axis=0)    centered = cats - mean_cat    centered_flat = centered.reshape(len(cats), -1)        # Perform SVD    U, S, Vt = np.linalg.svd(centered_flat.T, full_matrices=False)        return S, U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EVD and SVD Relationship\n",
        "\n",
        "The singular values from SVD are related to eigenvalues from EVD by the formula: eigenvalues = (singular_values)^2 / (n-1). Both methods should produce equivalent principal components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Reconstruction\n",
        "\n",
        "This section reconstructs cat faces using varying numbers of singular values (10, 40, and 80) to demonstrate dimensionality reduction effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reconstruct_cats(cats, eigenvectors, n_components):\n",
        "    \"\"\"Reconstruct images using n_components eigenvectors.\n",
        "    \n",
        "    Args:\n",
        "        cats: Original images (n_images, height, width)\n",
        "        eigenvectors: Principal components (n_pixels, n_components)\n",
        "        n_components: Number of components to use\n",
        "    \n",
        "    Returns:\n",
        "        Reconstructed images (n_images, height, width)\n",
        "    \"\"\"\n",
        "    mean_cat = cats.mean(axis=0)\n",
        "    centered = cats - mean_cat\n",
        "    centered_flat = centered.reshape(len(cats), -1)\n",
        "    \n",
        "    # Project onto principal components and back\n",
        "    components = eigenvectors[:, :n_components]\n",
        "    projected = centered_flat @ components\n",
        "    reconstructed_flat = projected @ components.T\n",
        "    \n",
        "    # Add mean back and reshape\n",
        "    reconstructed = reconstructed_flat.reshape(cats.shape) + mean_cat\n",
        "    return reconstructed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Reconstruction with different component counts\n",
        "\n",
        "Demonstrates the trade-off between compression (fewer components) and reconstruction quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder for future reconstruction demo\n",
        "# for n in [10, 40, 80]:\n",
        "#     reconstructed = reconstruct_cats(cats, eigenvectors, n)\n",
        "#     plot_cats(reconstructed[:4], title=f'Reconstructed with {n} components')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analysis: Reconstruction Quality vs Compression\n",
        "\n",
        "With 10 components: High compression but noticeable quality loss. Good for rough identification.\n",
        "\n",
        "With 40 components: Good balance between compression and quality. Most facial features preserved.\n",
        "\n",
        "With 80 components: Near-perfect reconstruction using all available components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis: Why EVD and SVD Produce Equivalent Results\n",
        "\n",
        "The EVD and SVD approaches produce equivalent results because they are mathematically related:\n",
        "\n",
        "- **EVD** operates on the covariance matrix C = X^T X / (n-1), finding eigenvectors and eigenvalues\n",
        "- **SVD** decomposes the data matrix X directly into X = U \u03a3 V^T\n",
        "- The right singular vectors V are the eigenvectors of X^T X\n",
        "- The eigenvalues \u03bb relate to singular values \u03c3 by: \u03bb = \u03c3\u00b2 / (n-1)\n",
        "\n",
        "Both methods extract the same principal components, just through different computational paths. SVD is often preferred for numerical stability."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}